{
  "url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
  "data": {
    "markdown": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40mridulrao674385%2Flanguage-modelling-on-mps-using-pytorch-044a2dfd9f62&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40mridulrao674385%2Flanguage-modelling-on-mps-using-pytorch-044a2dfd9f62&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Language Modelling on MPS using PyTorch\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:32:32/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---byline--044a2dfd9f62---------------------------------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---byline--044a2dfd9f62---------------------------------------)\n\nFollow\n\n6 min read\n\n·\n\nMar 16, 2024\n\n1\n\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D044a2dfd9f62&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mridulrao674385%2Flanguage-modelling-on-mps-using-pytorch-044a2dfd9f62&source=---header_actions--044a2dfd9f62---------------------post_audio_button------------------)\n\nShare\n\nLanguage modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human language accurately. Among the various techniques, masked language modeling (MLM) is effective in context understanding, where the model predicts missing words within a sentence without relying on the subsequent context. Masking of words is done so that the model does not have access to future words so that it can learn to predict those words.\n\nIn this article, we delve into implementing a masked language model using PyTorch, adopting a model and its functionality from the PyTorch documentation. We will focus on leveraging the Metal Performance Shaders (MPS) as an accelerator to reduce training times significantly. The model’s architecture is adopted from PyTorch’s transformer architecture. I aim to provide insights into optimizing performance and achieving faster, more efficient language modeling outcomes.\n\n> The detailed code can be viewed [here](https://github.com/mridulrao/MPS_training/blob/main/GPU.ipynb).\n\n**DATA**\n\nThe CNN/Daily Mail dataset is famous for natural language processing tasks, particularly in text summarization. It comprises numerous news articles from CNN and the Daily Mail, annotated with highlights as summaries. This dataset is widely used for training and evaluating language models, providing a rich source of varied and complex textual content that is ideal for understanding and generating human language.\n\nThe text needs a bit of preprocessing, but we will not be doing it much so that the natural context of a text is preserved.\n\n```\nimport re\nfrom datasets import load_dataset\n\ndata = load_dataset(\"cnn_dailymail\", \"2.0.0\", split = 'train')\n\ndef filter_data(text):\n    #remove last line\n    text = re.sub(r\"Copyright \\d{4} Reuters. All rights reserved.*\", \"\", text)\n\n    #replace \\'\n    text = text.replace(\"\\'\", \"\")\n\n    #replace 's\n    text = re.sub(r\"'s\\b'\", \"\", text)\n\n    #remove extra white space\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n```\n\n**MODEL**\n\nIn our project, we leverage the t [ransformer architecture from PyTorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html), explicitly utilizing the Transformer encoder layer to build our language model. This approach allows us to benefit from built-in self-attention mechanisms, simplifying the model construction process without implementing these components from scratch. By stacking multiple Transformer encoder layers and pairing them with a simple linear decoder, our model is structured to predict the next word in a sequence.\n\n```\n#model\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntokens, ninp, nhead, nhid, nlayers, dropout = 0.5):\n        super(TransformerModel, self).__init__()\n        self.model_type = \"Transformer\"\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layer = TransformerEncoderLayer(ninp, nhead, nhid, dropout, batch_first = True)\n        self.transformer_encoder = TransformerEncoder(encoder_layer, nlayers)\n        self.encoder = nn.Embedding(ntokens, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntokens)\n\n        self.init_weights()\n\n    def generate_square_subsequent_mask(self, sz):\n\n        '''\n        We generate the mask to prevent the transformer from seeing future tokens\n        Square matrix is created with elements below the diagonal = 0\n        Conver the mask to float, all zeros are replaced with -inf(indicating no access to elements)\n        and 1 with 0.0(this apporation does not changes the magnitude but influences the output)\n        '''\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src, src_mask):\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n```\n\nWe adopted a masked language modeling strategy(from Pytorch) where 15% of the input tokens are randomly replaced with a \\[MASK\\] token, and the model’s objective is to predict these masked tokens accurately. This method is beneficial during pre-training, as it aligns with real-world scenarios where explicit hints (\\[MASK\\] tokens) are absent. 10% of the masked tokens are kept as original, and another 10% are replaced with random tokens. This training approach teaches the model to verify the correctness of unaltered inputs and deduce accurate predictions, refining its predictive accuracy and contextual understanding.\n\n```\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n```\n\n**MPS**\n\nNext, we check the availability of MPS to ensure we can harness its power for faster machine learning. If MPS is ready, we set up our neural network model with specific parameters, gearing it up for efficient training on this advanced hardware. This step is crucial for optimizing performance and getting the most out of the machine’s capabilities.\n\nThe setup of training arguments using the transformers library is given below. It is essential to define how the model will learn. We create a TrainingArguments object, specifying key parameters that will influence the training process:\n\n```\nmps_device = torch.device(\"mps\")\n\nntokens = tokenizer.vocab_size\nemsize = 512\nnhid = 100\nnlayers = 5\nnhead = 4\ndropout = 0.2\n\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(mps_device)\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1,\n    learning_rate = 0.1,\n    gradient_accumulation_steps=8,\n    #gradient_checkpointing=True, # transformer models dont have this feature\n    #fp16=True, # can only be done with CUDA\n    output_dir = \"./model_output\"\n)\n```\n\nper\\_device\\_train\\_batch\\_size sets the batch size to 1 for each device during training, meaning each step of the training process will use only one example at a time. It is a crucial setting that affects memory usage and training speed.\n\nlearning\\_rate is set to 0.1, determining how rapidly the model updates its knowledge during training. A higher rate speeds up learning but can overshoot the optimal point.\n\ngradient\\_accumulation\\_steps allows us to accumulate the gradients over eight steps before performing a backpropagation update. It is useful for training with larger batch sizes without needing more memory.\n\n## Get Mridul Rao’s stories in your inbox\n\nJoin Medium for free to get updates from this writer.\n\nSubscribe\n\nSubscribe\n\nBy defining these arguments, we tailor the training process to balance efficiency and resource usage, setting the stage for effective model learning on MPS-enabled devices.\n\nThen, we organize the model parameters into groups to optimize them effectively during training, focusing on applying weight decay correctly. First, it identifies which parameters should and should not receive weight decay — this is important for regularizing the model without hampering elements like biases or normalization parameters that do not benefit from such regularization.\n\n```\nfrom transformers.trainer_pt_utils import get_parameter_names\n\ndecay_parameters = get_parameter_names(model, [nn.LayerNorm])\ndecay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n\noptimizer_grouped_parameters = [\\\n    {\\\n        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\\\n        \"weight_decay\": training_args.weight_decay,\\\n    },\\\n    {\\\n        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\\\n        \"weight_decay\": 0.0,\\\n    },\\\n]\n\noptimizer_kwargs = {\n    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n    \"eps\": training_args.adam_epsilon,\n}\n\noptim = torch.optim.AdamW(optimizer_grouped_parameters,\n                        lr = training_args.learning_rate,\n                        betas=(training_args.adam_beta1, training_args.adam_beta2),\n                        eps=training_args.adam_epsilon,)\n```\n\nThe parameters are then split into two groups. One group gets the specified weight decay, helping to prevent overfitting by penalizing only the weights. The other group, which includes biases and specific other parameters, does not receive weight decay to maintain their necessary roles without unnecessary penalization.\n\nLastly, the optimizer’s settings, such as the betas and eps values for the Adam optimizer, are fine-tuned. These settings adjust how the optimizer averages past gradients and maintains numerical stability, which is essential for smooth and effective model training. This careful organization and optimization setup are vital for efficiently training the model.\n\n**TRAINING**\n\nFor the model training, we used cross-entropy and trained the model for 30 epochs. The training loop is pretty self-explanatory, summarized below -\n\nThe input tensor is prepared by cloning input\\_ids from the batch.\n\nA source mask (src\\_mask) is generated to help the model focus on relevant parts of the input sequence.\n\nRandom masking is applied: a mask is generated to select tokens for replacement, avoiding special tokens randomly. The selected tokens (15% chance) are replaced with the \\[MASK\\] token.\n\nThe model makes predictions based on the masked input and the source mask.\n\nThe loss is calculated by comparing the model’s output against the actual input ids, considering the entire vocabulary size (ntokens).\n\nThe accelerator.backward(loss) call computes the gradient of the loss concerning the model parameters.\n\nLoss values are accumulated, and once every few steps (defined by gradient\\_accumulation\\_steps), the model’s weights are updated using the optimizer.\n\nWhen the condition step condition is met, optimizer.step() applies the computed gradients to update the model weights, and optimizer.zero\\_grad() clears old gradients to prepare for the next step.\n\n```\ndataloader = DataLoader(train_data, batch_size=training_args.per_device_train_batch_size, collate_fn = data_collate)\n\naccelerator = Accelerator()\nmodel, optimizer, dataloader = accelerator.prepare(model, optim, dataloader)\n\nmodel.train()\ncriterion = nn.CrossEntropyLoss()\ntotal_loss = 0\nepochs = 30\n\nfor epoch in range(epochs):\n    for step, batch in tqdm(enumerate(dataloader, start=1)):\n        #prepare input\n        input = batch['input_ids'].\n        src_mask = model.generate_square_subsequent_mask(batch['input_ids'].size(1))\n\n        #genearate mask for random values\n        rand_value = torch.rand(batch.input_ids.shape)\n\n        rand_mask = (rand_value.to(mps_device) < 0.15) * (input != 101) * (input != 102) * (input != 0)\n\n        #store masked index\n        mask_idx=(rand_mask.flatten() == True).nonzero().view(-1)\n\n        input = input.flatten()\n        input[mask_idx] = 103\n        input = input.view(batch['input_ids'].size())\n\n        out = model(input.to(mps_device), src_mask.to(mps_device))\n\n        loss = criterion(out.view(-1, ntokens), batch['input_ids'].view(-1).to(mps_device))\n        total_loss += loss\n\n        accelerator.backward(loss)\n\n        if step % training_args.gradient_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n    print(total_loss/(len(dataloader)*epoch+1))\n```\n\n**CONCLUSION**\n\nHere we covered how we can use MPS at its capacity to train a language model. Changing and trying different hyper-parameters may improve better results too which can be explored. The detailed code can be viewed on this [github\\_repo](https://github.com/mridulrao/MPS_training/blob/main/GPU.ipynb). Thank you for reading!\n\nRESOURCES and REFERENCES\n\n1. [https://pytorch.org/docs/stable/\\_modules/torch/nn/modules/transformer.html#Transformer.generate\\_square\\_subsequent\\_mask](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer.generate_square_subsequent_mask)\n2. [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)\n3. [https://huggingface.co/datasets/cnn\\_dailymail](https://huggingface.co/datasets/cnn_dailymail)\n4. [https://pytorch.org/tutorials/beginner/transformer\\_tutorial.html](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n5. [https://pytorch.org/docs/stable/notes/mps.html](https://pytorch.org/docs/stable/notes/mps.html)\n6. [https://pytorch.org/docs/stable/notes/autograd.html](https://pytorch.org/docs/stable/notes/autograd.html)\n7. [https://huggingface.co/docs/transformers/en/perf\\_train\\_gpu\\_one](https://huggingface.co/docs/transformers/en/perf_train_gpu_one)\n8. [https://huggingface.co/docs/transformers/v4.18.0/en/performance](https://huggingface.co/docs/transformers/v4.18.0/en/performance)\n\n[Language Model](https://medium.com/tag/language-model?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Python](https://medium.com/tag/python?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Pytorch](https://medium.com/tag/pytorch?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Deep Learning](https://medium.com/tag/deep-learning?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:48:48/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:64:64/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\nFollow\n\n[**Written by Mridul Rao**](https://medium.com/@mridulrao674385?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\n[21 followers](https://medium.com/@mridulrao674385/followers?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\n· [13 following](https://medium.com/@mridulrao674385/following?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\nML/AI Enthusiast\n\nFollow\n\n## No responses yet\n\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\n\nWrite a response\n\n[What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mridulrao674385%2Flanguage-modelling-on-mps-using-pytorch-044a2dfd9f62&source=---post_responses--044a2dfd9f62---------------------respond_sidebar------------------)\n\nCancel\n\nRespond\n\n## More from Mridul Rao\n\n![Attention Mechanism Complexity Analysis](https://miro.medium.com/v2/resize:fit:679/format:webp/0*Re796fmjO90us8op.png)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:20:20/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----0---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----0---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[**Attention Mechanism Complexity Analysis**\\\\\n\\\\\n**Overview of why the attention mechanism is expensive and what people do to speed it up.**](https://medium.com/@mridulrao674385/attention-mechanism-complexity-analysis-7314063459b1?source=post_page---author_recirc--044a2dfd9f62----0---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\nJan 26, 2025\n\n[A clap icon27\\\\\n\\\\\nA response icon1](https://medium.com/@mridulrao674385/attention-mechanism-complexity-analysis-7314063459b1?source=post_page---author_recirc--044a2dfd9f62----0---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n![Deploying SLM for building AI Agents](https://miro.medium.com/v2/resize:fit:679/format:webp/1*kDzKY_NvP_2khVm0aXUkfg.jpeg)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:20:20/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----1---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----1---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[**Deploying SLM for building AI Agents**\\\\\n\\\\\n**In this article, I will walk through the practical steps and standard practices for deploying small language models (SLMs in the 4B–7B…**](https://medium.com/@mridulrao674385/deploying-slm-for-building-ai-agents-742fa5cfdbd4?source=post_page---author_recirc--044a2dfd9f62----1---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\nDec 6, 2025\n\n[A clap icon4](https://medium.com/@mridulrao674385/deploying-slm-for-building-ai-agents-742fa5cfdbd4?source=post_page---author_recirc--044a2dfd9f62----1---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n![Matrix Decomposition — SVD](https://miro.medium.com/v2/resize:fit:679/format:webp/0*aHPDCXw1y8SupOKt.jpg)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:20:20/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----2---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----2---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[**Matrix Decomposition — SVD**\\\\\n\\\\\n**Continuing our series on Matrix Decomposition, in this post, we will explore Singular Value Decomposition (SVD). SVD is one of the most…**](https://medium.com/@mridulrao674385/matrix-decomposition-svd-90218d832b4d?source=post_page---author_recirc--044a2dfd9f62----2---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\nSep 15, 2024\n\n[A clap icon53](https://medium.com/@mridulrao674385/matrix-decomposition-svd-90218d832b4d?source=post_page---author_recirc--044a2dfd9f62----2---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n![Building Voice Agents: Understanding Telephony Infrastructure](https://miro.medium.com/v2/resize:fit:679/format:webp/1*q9YzHD32Hz1wcM2ayp836Q.png)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:20:20/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----3---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----3---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[**Building Voice Agents: Understanding Telephony Infrastructure**\\\\\n\\\\\n**Voice agents have come a long way — from traditional landline systems to modern, real-time communication platforms. Today’s telephony…**](https://medium.com/@mridulrao674385/building-voice-agents-understanding-telephony-infrastructure-2236c09d807a?source=post_page---author_recirc--044a2dfd9f62----3---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\nMar 23, 2025\n\n[A clap icon1](https://medium.com/@mridulrao674385/building-voice-agents-understanding-telephony-infrastructure-2236c09d807a?source=post_page---author_recirc--044a2dfd9f62----3---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[See all from Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62---------------------------------------)\n\n## Recommended from Medium\n\n![AI Agents: Complete Course](https://miro.medium.com/v2/resize:fit:679/format:webp/1*PvPPSGJ9779FTWmtK_Yeyw.png)\n\n[![Data Science Collective](https://miro.medium.com/v2/resize:fill:20:20/1*0nV0Q-FBHj94Kggq00pG2Q.jpeg)](https://medium.com/data-science-collective?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Data Science Collective](https://medium.com/data-science-collective?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Marina Wyss - Gratitude Driven](https://medium.com/@gratitudedriven?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**AI Agents: Complete Course**\\\\\n\\\\\n**From beginner to intermediate to production.**](https://medium.com/data-science-collective/ai-agents-complete-course-f226aa4550a1?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nDec 6, 2025\n\n[A clap icon2.8K\\\\\n\\\\\nA response icon94](https://medium.com/data-science-collective/ai-agents-complete-course-f226aa4550a1?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![Stanford Just Killed Prompt Engineering With 8 Words (And I Can’t Believe It Worked)](https://miro.medium.com/v2/resize:fit:679/format:webp/1*va3sFwIm26snbj5ly9ZsgA.jpeg)\n\n[![Generative AI](https://miro.medium.com/v2/resize:fill:20:20/1*M4RBhIRaSSZB7lXfrGlatA.png)](https://medium.com/generative-ai?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Generative AI](https://medium.com/generative-ai?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Adham Khaled](https://medium.com/@adham__khaled__?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**Stanford Just Killed Prompt Engineering With 8 Words (And I Can’t Believe It Worked)**\\\\\n\\\\\n**ChatGPT keeps giving you the same boring response? This new technique unlocks 2× more creativity from ANY AI model — no training required…**](https://medium.com/generative-ai/stanford-just-killed-prompt-engineering-with-8-words-and-i-cant-believe-it-worked-8349d6524d2b?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nOct 19, 2025\n\n[A clap icon23K\\\\\n\\\\\nA response icon590](https://medium.com/generative-ai/stanford-just-killed-prompt-engineering-with-8-words-and-i-cant-believe-it-worked-8349d6524d2b?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![The AI Bubble Is About To Burst, But The Next Bubble Is Already Growing](https://miro.medium.com/v2/resize:fit:679/format:webp/0*jQ7Z0Y2Rw8kblsEX)\n\n[![Will Lockett](https://miro.medium.com/v2/resize:fill:20:20/1*V0qWMQ8V5_NaF9yUoHAdyg.jpeg)](https://medium.com/@wlockett?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Will Lockett](https://medium.com/@wlockett?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**The AI Bubble Is About To Burst, But The Next Bubble Is Already Growing**\\\\\n\\\\\n**Techbros are preparing their latest bandwagon.**](https://medium.com/@wlockett/the-ai-bubble-is-about-to-burst-but-the-next-bubble-is-already-growing-383c0c0c7ede?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nSep 14, 2025\n\n[A clap icon21K\\\\\n\\\\\nA response icon918](https://medium.com/@wlockett/the-ai-bubble-is-about-to-burst-but-the-next-bubble-is-already-growing-383c0c0c7ede?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![Building the 7 Layers of a Production-Grade Agentic AI System](https://miro.medium.com/v2/resize:fit:679/format:webp/1*GB6tXauVBaHVGDE4L_FkYg.png)\n\n[![Level Up Coding](https://miro.medium.com/v2/resize:fill:20:20/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg)](https://medium.com/gitconnected?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Level Up Coding](https://medium.com/gitconnected?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Fareed Khan](https://medium.com/@fareedkhandev?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**Building the 7 Layers of a Production-Grade Agentic AI System**\\\\\n\\\\\n**Service Layer, Middleware, Context Management and more**](https://medium.com/gitconnected/building-the-7-layers-of-a-production-grade-agentic-ai-system-37ee5d941f1c?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nDec 18, 2025\n\n[A clap icon1.5K\\\\\n\\\\\nA response icon21](https://medium.com/gitconnected/building-the-7-layers-of-a-production-grade-agentic-ai-system-37ee5d941f1c?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![What are Context Graphs: Building the AI that trulyUnderstands](https://miro.medium.com/v2/resize:fit:679/format:webp/0*KKPAVRJmt0f9PdOC)\n\n[![Neural Notions](https://miro.medium.com/v2/resize:fill:20:20/1*Xv_4SeKtNfCiqRNi_wDAoA.png)](https://medium.com/modelmind?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Neural Notions](https://medium.com/modelmind?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Nikhil](https://medium.com/@nkwrites?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**What are Context Graphs: Building the AI that trulyUnderstands**\\\\\n\\\\\n**Imagine an AI system that not only processes information but truly understands it -> grasping the relationships between entities, the…**](https://medium.com/modelmind/what-are-context-graphs-building-the-ai-that-trulyunderstands-e7e5db39138d?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nDec 24, 2025\n\n[A clap icon33](https://medium.com/modelmind/what-are-context-graphs-building-the-ai-that-trulyunderstands-e7e5db39138d?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![Git Confused Me for Years Until I Found This Simple Guide](https://miro.medium.com/v2/resize:fit:679/format:webp/1*YUALkK55VO_6mxjVqq_smQ.png)\n\n[![Let’s Code Future](https://miro.medium.com/v2/resize:fill:20:20/1*QXfeVFVbIzUGnlwXoOZvyQ.png)](https://medium.com/lets-code-future?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Let’s Code Future](https://medium.com/lets-code-future?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[The Unwritten Algorithm](https://medium.com/@the_unwritten_algorithm?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**Git Confused Me for Years Until I Found This Simple Guide**\\\\\n\\\\\n**Most developers don’t really understand Git — here’s the simple truth.**](https://medium.com/lets-code-future/git-confused-me-for-years-until-i-found-this-simple-guide-a45223bebb40?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nDec 19, 2025\n\n[A clap icon2.7K\\\\\n\\\\\nA response icon55](https://medium.com/lets-code-future/git-confused-me-for-years-until-i-found-this-simple-guide-a45223bebb40?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[See more recommendations](https://medium.com/?source=post_page---read_next_recirc--044a2dfd9f62---------------------------------------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Status](https://status.medium.com/?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----044a2dfd9f62---------------------------------------)\n\nreCAPTCHA\n\nRecaptcha requires verification.\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nprotected by **reCAPTCHA**\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)"
  },
  "metadata": {
    "title": "Language Modelling on MPS using PyTorch | by Mridul Rao | Medium",
    "description": "Language Modelling on MPS using PyTorch Language modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human language accurately …",
    "url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
    "language": "en",
    "keywords": null,
    "robots": "index,noarchive,follow,max-image-preview:large",
    "og_title": "Language Modelling on MPS using PyTorch",
    "og_description": "Language modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human…",
    "og_url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
    "og_image": "https://miro.medium.com/v2/da:true/bc1f8416df0cad099e43cda2872716e5864f18a73bda2a7547ea082aca9b5632",
    "og_audio": null,
    "og_determiner": null,
    "og_locale": null,
    "og_locale_alternate": null,
    "og_site_name": "Medium",
    "og_video": null,
    "favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
    "dc_terms_created": null,
    "dc_date_created": null,
    "dc_date": null,
    "dc_terms_type": null,
    "dc_type": null,
    "dc_terms_audience": null,
    "dc_terms_subject": null,
    "dc_subject": null,
    "dc_description": null,
    "dc_terms_keywords": null,
    "modified_time": null,
    "published_time": "2024-03-23T18:13:00.346Z",
    "article_tag": null,
    "article_section": null,
    "source_url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
    "status_code": 200,
    "scrape_id": "019bfe68-6a67-72aa-9edf-85819f5be6d2",
    "num_pages": null,
    "content_type": "text/html; charset=utf-8",
    "proxy_used": "basic",
    "timezone": null,
    "cache_state": "hit",
    "cached_at": "2026-01-27T04:00:44.439Z",
    "credits_used": 1,
    "concurrency_limited": false,
    "concurrency_queue_duration_ms": null,
    "error": null,
    "og:image:alt": "Medium",
    "twitter:data1": "6 min read",
    "referrer": "unsafe-url",
    "al:ios:url": "medium://p/044a2dfd9f62",
    "al:web:url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
    "twitter:app:url:iphone": "medium://p/044a2dfd9f62",
    "al:android:package": "com.medium.reader",
    "twitter:card": "summary_large_image",
    "viewport": "width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1",
    "og:image": "https://miro.medium.com/v2/da:true/bc1f8416df0cad099e43cda2872716e5864f18a73bda2a7547ea082aca9b5632",
    "twitter:title": "Language Modelling on MPS using PyTorch",
    "al:ios:app_name": "Medium",
    "fb:app_id": "542599432471018",
    "og:type": "article",
    "author": "Mridul Rao",
    "al:android:app_name": "Medium",
    "article:published_time": "2024-03-23T18:13:00.346Z",
    "og:title": "Language Modelling on MPS using PyTorch",
    "twitter:site": "@Medium",
    "og:description": "Language modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human…",
    "al:ios:app_store_id": "828256236",
    "twitter:image:src": "https://miro.medium.com/v2/da:true/bc1f8416df0cad099e43cda2872716e5864f18a73bda2a7547ea082aca9b5632",
    "twitter:app:id:iphone": "828256236",
    "twitter:app:name:iphone": "Medium",
    "theme-color": "#000000",
    "al:android:url": "medium://p/044a2dfd9f62",
    "twitter:image:alt": "Medium",
    "article:author": "https://medium.com/@mridulrao674385",
    "og:url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
    "og:site_name": "Medium",
    "apple-itunes-app": "app-id=828256236, app-argument=/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62, affiliate-data=pt=698524&ct=smart_app_banner&mt=8",
    "twitter:description": "Language modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human…",
    "twitter:label1": "Reading time"
  },
  "raw": {
    "markdown": "[Sitemap](https://medium.com/sitemap/sitemap.xml)\n\n[Open in app](https://play.google.com/store/apps/details?id=com.medium.reader&referrer=utm_source%3DmobileNavBar&source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40mridulrao674385%2Flanguage-modelling-on-mps-using-pytorch-044a2dfd9f62&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n[Medium Logo](https://medium.com/?source=post_page---top_nav_layout_nav-----------------------------------------)\n\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---top_nav_layout_nav-----------------------new_post_topnav------------------)\n\n[Search](https://medium.com/search?source=post_page---top_nav_layout_nav-----------------------------------------)\n\nSign up\n\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fmedium.com%2F%40mridulrao674385%2Flanguage-modelling-on-mps-using-pytorch-044a2dfd9f62&source=post_page---top_nav_layout_nav-----------------------global_nav------------------)\n\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\n\n# Language Modelling on MPS using PyTorch\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:32:32/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---byline--044a2dfd9f62---------------------------------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---byline--044a2dfd9f62---------------------------------------)\n\nFollow\n\n6 min read\n\n·\n\nMar 16, 2024\n\n1\n\n[Listen](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2Fplans%3Fdimension%3Dpost_audio_button%26postId%3D044a2dfd9f62&operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mridulrao674385%2Flanguage-modelling-on-mps-using-pytorch-044a2dfd9f62&source=---header_actions--044a2dfd9f62---------------------post_audio_button------------------)\n\nShare\n\nLanguage modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human language accurately. Among the various techniques, masked language modeling (MLM) is effective in context understanding, where the model predicts missing words within a sentence without relying on the subsequent context. Masking of words is done so that the model does not have access to future words so that it can learn to predict those words.\n\nIn this article, we delve into implementing a masked language model using PyTorch, adopting a model and its functionality from the PyTorch documentation. We will focus on leveraging the Metal Performance Shaders (MPS) as an accelerator to reduce training times significantly. The model’s architecture is adopted from PyTorch’s transformer architecture. I aim to provide insights into optimizing performance and achieving faster, more efficient language modeling outcomes.\n\n> The detailed code can be viewed [here](https://github.com/mridulrao/MPS_training/blob/main/GPU.ipynb).\n\n**DATA**\n\nThe CNN/Daily Mail dataset is famous for natural language processing tasks, particularly in text summarization. It comprises numerous news articles from CNN and the Daily Mail, annotated with highlights as summaries. This dataset is widely used for training and evaluating language models, providing a rich source of varied and complex textual content that is ideal for understanding and generating human language.\n\nThe text needs a bit of preprocessing, but we will not be doing it much so that the natural context of a text is preserved.\n\n```\nimport re\nfrom datasets import load_dataset\n\ndata = load_dataset(\"cnn_dailymail\", \"2.0.0\", split = 'train')\n\ndef filter_data(text):\n    #remove last line\n    text = re.sub(r\"Copyright \\d{4} Reuters. All rights reserved.*\", \"\", text)\n\n    #replace \\'\n    text = text.replace(\"\\'\", \"\")\n\n    #replace 's\n    text = re.sub(r\"'s\\b'\", \"\", text)\n\n    #remove extra white space\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text\n```\n\n**MODEL**\n\nIn our project, we leverage the t [ransformer architecture from PyTorch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html), explicitly utilizing the Transformer encoder layer to build our language model. This approach allows us to benefit from built-in self-attention mechanisms, simplifying the model construction process without implementing these components from scratch. By stacking multiple Transformer encoder layers and pairing them with a simple linear decoder, our model is structured to predict the next word in a sequence.\n\n```\n#model\nclass TransformerModel(nn.Module):\n\n    def __init__(self, ntokens, ninp, nhead, nhid, nlayers, dropout = 0.5):\n        super(TransformerModel, self).__init__()\n        self.model_type = \"Transformer\"\n        self.pos_encoder = PositionalEncoding(ninp, dropout)\n        encoder_layer = TransformerEncoderLayer(ninp, nhead, nhid, dropout, batch_first = True)\n        self.transformer_encoder = TransformerEncoder(encoder_layer, nlayers)\n        self.encoder = nn.Embedding(ntokens, ninp)\n        self.ninp = ninp\n        self.decoder = nn.Linear(ninp, ntokens)\n\n        self.init_weights()\n\n    def generate_square_subsequent_mask(self, sz):\n\n        '''\n        We generate the mask to prevent the transformer from seeing future tokens\n        Square matrix is created with elements below the diagonal = 0\n        Conver the mask to float, all zeros are replaced with -inf(indicating no access to elements)\n        and 1 with 0.0(this apporation does not changes the magnitude but influences the output)\n        '''\n        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n\n        return mask\n\n    def init_weights(self):\n        initrange = 0.1\n        self.encoder.weight.data.uniform_(-initrange, initrange)\n        self.decoder.bias.data.zero_()\n        self.decoder.weight.data.uniform_(-initrange, initrange)\n\n    def forward(self, src, src_mask):\n        src = self.encoder(src) * math.sqrt(self.ninp)\n        src = self.pos_encoder(src)\n\n        output = self.transformer_encoder(src, src_mask)\n        output = self.decoder(output)\n        return output\n```\n\nWe adopted a masked language modeling strategy(from Pytorch) where 15% of the input tokens are randomly replaced with a \\[MASK\\] token, and the model’s objective is to predict these masked tokens accurately. This method is beneficial during pre-training, as it aligns with real-world scenarios where explicit hints (\\[MASK\\] tokens) are absent. 10% of the masked tokens are kept as original, and another 10% are replaced with random tokens. This training approach teaches the model to verify the correctness of unaltered inputs and deduce accurate predictions, refining its predictive accuracy and contextual understanding.\n\n```\nclass PositionalEncoding(nn.Module):\n\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[:x.size(0), :]\n        return self.dropout(x)\n```\n\n**MPS**\n\nNext, we check the availability of MPS to ensure we can harness its power for faster machine learning. If MPS is ready, we set up our neural network model with specific parameters, gearing it up for efficient training on this advanced hardware. This step is crucial for optimizing performance and getting the most out of the machine’s capabilities.\n\nThe setup of training arguments using the transformers library is given below. It is essential to define how the model will learn. We create a TrainingArguments object, specifying key parameters that will influence the training process:\n\n```\nmps_device = torch.device(\"mps\")\n\nntokens = tokenizer.vocab_size\nemsize = 512\nnhid = 100\nnlayers = 5\nnhead = 4\ndropout = 0.2\n\nmodel = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(mps_device)\n\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=1,\n    learning_rate = 0.1,\n    gradient_accumulation_steps=8,\n    #gradient_checkpointing=True, # transformer models dont have this feature\n    #fp16=True, # can only be done with CUDA\n    output_dir = \"./model_output\"\n)\n```\n\nper\\_device\\_train\\_batch\\_size sets the batch size to 1 for each device during training, meaning each step of the training process will use only one example at a time. It is a crucial setting that affects memory usage and training speed.\n\nlearning\\_rate is set to 0.1, determining how rapidly the model updates its knowledge during training. A higher rate speeds up learning but can overshoot the optimal point.\n\ngradient\\_accumulation\\_steps allows us to accumulate the gradients over eight steps before performing a backpropagation update. It is useful for training with larger batch sizes without needing more memory.\n\n## Get Mridul Rao’s stories in your inbox\n\nJoin Medium for free to get updates from this writer.\n\nSubscribe\n\nSubscribe\n\nBy defining these arguments, we tailor the training process to balance efficiency and resource usage, setting the stage for effective model learning on MPS-enabled devices.\n\nThen, we organize the model parameters into groups to optimize them effectively during training, focusing on applying weight decay correctly. First, it identifies which parameters should and should not receive weight decay — this is important for regularizing the model without hampering elements like biases or normalization parameters that do not benefit from such regularization.\n\n```\nfrom transformers.trainer_pt_utils import get_parameter_names\n\ndecay_parameters = get_parameter_names(model, [nn.LayerNorm])\ndecay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n\noptimizer_grouped_parameters = [\\\n    {\\\n        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\\\n        \"weight_decay\": training_args.weight_decay,\\\n    },\\\n    {\\\n        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\\\n        \"weight_decay\": 0.0,\\\n    },\\\n]\n\noptimizer_kwargs = {\n    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n    \"eps\": training_args.adam_epsilon,\n}\n\noptim = torch.optim.AdamW(optimizer_grouped_parameters,\n                        lr = training_args.learning_rate,\n                        betas=(training_args.adam_beta1, training_args.adam_beta2),\n                        eps=training_args.adam_epsilon,)\n```\n\nThe parameters are then split into two groups. One group gets the specified weight decay, helping to prevent overfitting by penalizing only the weights. The other group, which includes biases and specific other parameters, does not receive weight decay to maintain their necessary roles without unnecessary penalization.\n\nLastly, the optimizer’s settings, such as the betas and eps values for the Adam optimizer, are fine-tuned. These settings adjust how the optimizer averages past gradients and maintains numerical stability, which is essential for smooth and effective model training. This careful organization and optimization setup are vital for efficiently training the model.\n\n**TRAINING**\n\nFor the model training, we used cross-entropy and trained the model for 30 epochs. The training loop is pretty self-explanatory, summarized below -\n\nThe input tensor is prepared by cloning input\\_ids from the batch.\n\nA source mask (src\\_mask) is generated to help the model focus on relevant parts of the input sequence.\n\nRandom masking is applied: a mask is generated to select tokens for replacement, avoiding special tokens randomly. The selected tokens (15% chance) are replaced with the \\[MASK\\] token.\n\nThe model makes predictions based on the masked input and the source mask.\n\nThe loss is calculated by comparing the model’s output against the actual input ids, considering the entire vocabulary size (ntokens).\n\nThe accelerator.backward(loss) call computes the gradient of the loss concerning the model parameters.\n\nLoss values are accumulated, and once every few steps (defined by gradient\\_accumulation\\_steps), the model’s weights are updated using the optimizer.\n\nWhen the condition step condition is met, optimizer.step() applies the computed gradients to update the model weights, and optimizer.zero\\_grad() clears old gradients to prepare for the next step.\n\n```\ndataloader = DataLoader(train_data, batch_size=training_args.per_device_train_batch_size, collate_fn = data_collate)\n\naccelerator = Accelerator()\nmodel, optimizer, dataloader = accelerator.prepare(model, optim, dataloader)\n\nmodel.train()\ncriterion = nn.CrossEntropyLoss()\ntotal_loss = 0\nepochs = 30\n\nfor epoch in range(epochs):\n    for step, batch in tqdm(enumerate(dataloader, start=1)):\n        #prepare input\n        input = batch['input_ids'].\n        src_mask = model.generate_square_subsequent_mask(batch['input_ids'].size(1))\n\n        #genearate mask for random values\n        rand_value = torch.rand(batch.input_ids.shape)\n\n        rand_mask = (rand_value.to(mps_device) < 0.15) * (input != 101) * (input != 102) * (input != 0)\n\n        #store masked index\n        mask_idx=(rand_mask.flatten() == True).nonzero().view(-1)\n\n        input = input.flatten()\n        input[mask_idx] = 103\n        input = input.view(batch['input_ids'].size())\n\n        out = model(input.to(mps_device), src_mask.to(mps_device))\n\n        loss = criterion(out.view(-1, ntokens), batch['input_ids'].view(-1).to(mps_device))\n        total_loss += loss\n\n        accelerator.backward(loss)\n\n        if step % training_args.gradient_accumulation_steps == 0:\n            optimizer.step()\n            optimizer.zero_grad()\n\n    print(total_loss/(len(dataloader)*epoch+1))\n```\n\n**CONCLUSION**\n\nHere we covered how we can use MPS at its capacity to train a language model. Changing and trying different hyper-parameters may improve better results too which can be explored. The detailed code can be viewed on this [github\\_repo](https://github.com/mridulrao/MPS_training/blob/main/GPU.ipynb). Thank you for reading!\n\nRESOURCES and REFERENCES\n\n1. [https://pytorch.org/docs/stable/\\_modules/torch/nn/modules/transformer.html#Transformer.generate\\_square\\_subsequent\\_mask](https://pytorch.org/docs/stable/_modules/torch/nn/modules/transformer.html#Transformer.generate_square_subsequent_mask)\n2. [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)\n3. [https://huggingface.co/datasets/cnn\\_dailymail](https://huggingface.co/datasets/cnn_dailymail)\n4. [https://pytorch.org/tutorials/beginner/transformer\\_tutorial.html](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n5. [https://pytorch.org/docs/stable/notes/mps.html](https://pytorch.org/docs/stable/notes/mps.html)\n6. [https://pytorch.org/docs/stable/notes/autograd.html](https://pytorch.org/docs/stable/notes/autograd.html)\n7. [https://huggingface.co/docs/transformers/en/perf\\_train\\_gpu\\_one](https://huggingface.co/docs/transformers/en/perf_train_gpu_one)\n8. [https://huggingface.co/docs/transformers/v4.18.0/en/performance](https://huggingface.co/docs/transformers/v4.18.0/en/performance)\n\n[Language Model](https://medium.com/tag/language-model?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Python](https://medium.com/tag/python?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Pytorch](https://medium.com/tag/pytorch?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Deep Learning](https://medium.com/tag/deep-learning?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:48:48/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:64:64/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\nFollow\n\n[**Written by Mridul Rao**](https://medium.com/@mridulrao674385?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\n[21 followers](https://medium.com/@mridulrao674385/followers?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\n· [13 following](https://medium.com/@mridulrao674385/following?source=post_page---post_author_info--044a2dfd9f62---------------------------------------)\n\nML/AI Enthusiast\n\nFollow\n\n## No responses yet\n\n![](https://miro.medium.com/v2/resize:fill:32:32/1*dmbNkD5D-u45r44go_cf0g.png)\n\nWrite a response\n\n[What are your thoughts?](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2F%40mridulrao674385%2Flanguage-modelling-on-mps-using-pytorch-044a2dfd9f62&source=---post_responses--044a2dfd9f62---------------------respond_sidebar------------------)\n\nCancel\n\nRespond\n\n## More from Mridul Rao\n\n![Attention Mechanism Complexity Analysis](https://miro.medium.com/v2/resize:fit:679/format:webp/0*Re796fmjO90us8op.png)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:20:20/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----0---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----0---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[**Attention Mechanism Complexity Analysis**\\\\\n\\\\\n**Overview of why the attention mechanism is expensive and what people do to speed it up.**](https://medium.com/@mridulrao674385/attention-mechanism-complexity-analysis-7314063459b1?source=post_page---author_recirc--044a2dfd9f62----0---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\nJan 26, 2025\n\n[A clap icon27\\\\\n\\\\\nA response icon1](https://medium.com/@mridulrao674385/attention-mechanism-complexity-analysis-7314063459b1?source=post_page---author_recirc--044a2dfd9f62----0---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n![Deploying SLM for building AI Agents](https://miro.medium.com/v2/resize:fit:679/format:webp/1*kDzKY_NvP_2khVm0aXUkfg.jpeg)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:20:20/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----1---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----1---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[**Deploying SLM for building AI Agents**\\\\\n\\\\\n**In this article, I will walk through the practical steps and standard practices for deploying small language models (SLMs in the 4B–7B…**](https://medium.com/@mridulrao674385/deploying-slm-for-building-ai-agents-742fa5cfdbd4?source=post_page---author_recirc--044a2dfd9f62----1---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\nDec 6, 2025\n\n[A clap icon4](https://medium.com/@mridulrao674385/deploying-slm-for-building-ai-agents-742fa5cfdbd4?source=post_page---author_recirc--044a2dfd9f62----1---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n![Matrix Decomposition — SVD](https://miro.medium.com/v2/resize:fit:679/format:webp/0*aHPDCXw1y8SupOKt.jpg)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:20:20/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----2---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----2---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[**Matrix Decomposition — SVD**\\\\\n\\\\\n**Continuing our series on Matrix Decomposition, in this post, we will explore Singular Value Decomposition (SVD). SVD is one of the most…**](https://medium.com/@mridulrao674385/matrix-decomposition-svd-90218d832b4d?source=post_page---author_recirc--044a2dfd9f62----2---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\nSep 15, 2024\n\n[A clap icon53](https://medium.com/@mridulrao674385/matrix-decomposition-svd-90218d832b4d?source=post_page---author_recirc--044a2dfd9f62----2---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n![Building Voice Agents: Understanding Telephony Infrastructure](https://miro.medium.com/v2/resize:fit:679/format:webp/1*q9YzHD32Hz1wcM2ayp836Q.png)\n\n[![Mridul Rao](https://miro.medium.com/v2/resize:fill:20:20/1*fqpkGmaOA18zAoYh-Vw1hw.jpeg)](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----3---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62----3---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[**Building Voice Agents: Understanding Telephony Infrastructure**\\\\\n\\\\\n**Voice agents have come a long way — from traditional landline systems to modern, real-time communication platforms. Today’s telephony…**](https://medium.com/@mridulrao674385/building-voice-agents-understanding-telephony-infrastructure-2236c09d807a?source=post_page---author_recirc--044a2dfd9f62----3---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\nMar 23, 2025\n\n[A clap icon1](https://medium.com/@mridulrao674385/building-voice-agents-understanding-telephony-infrastructure-2236c09d807a?source=post_page---author_recirc--044a2dfd9f62----3---------------------b8170f48_ef8e_4283_8579_6f8a62c1d911--------------)\n\n[See all from Mridul Rao](https://medium.com/@mridulrao674385?source=post_page---author_recirc--044a2dfd9f62---------------------------------------)\n\n## Recommended from Medium\n\n![AI Agents: Complete Course](https://miro.medium.com/v2/resize:fit:679/format:webp/1*PvPPSGJ9779FTWmtK_Yeyw.png)\n\n[![Data Science Collective](https://miro.medium.com/v2/resize:fill:20:20/1*0nV0Q-FBHj94Kggq00pG2Q.jpeg)](https://medium.com/data-science-collective?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Data Science Collective](https://medium.com/data-science-collective?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Marina Wyss - Gratitude Driven](https://medium.com/@gratitudedriven?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**AI Agents: Complete Course**\\\\\n\\\\\n**From beginner to intermediate to production.**](https://medium.com/data-science-collective/ai-agents-complete-course-f226aa4550a1?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nDec 6, 2025\n\n[A clap icon2.8K\\\\\n\\\\\nA response icon94](https://medium.com/data-science-collective/ai-agents-complete-course-f226aa4550a1?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![Stanford Just Killed Prompt Engineering With 8 Words (And I Can’t Believe It Worked)](https://miro.medium.com/v2/resize:fit:679/format:webp/1*va3sFwIm26snbj5ly9ZsgA.jpeg)\n\n[![Generative AI](https://miro.medium.com/v2/resize:fill:20:20/1*M4RBhIRaSSZB7lXfrGlatA.png)](https://medium.com/generative-ai?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Generative AI](https://medium.com/generative-ai?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Adham Khaled](https://medium.com/@adham__khaled__?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**Stanford Just Killed Prompt Engineering With 8 Words (And I Can’t Believe It Worked)**\\\\\n\\\\\n**ChatGPT keeps giving you the same boring response? This new technique unlocks 2× more creativity from ANY AI model — no training required…**](https://medium.com/generative-ai/stanford-just-killed-prompt-engineering-with-8-words-and-i-cant-believe-it-worked-8349d6524d2b?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nOct 19, 2025\n\n[A clap icon23K\\\\\n\\\\\nA response icon590](https://medium.com/generative-ai/stanford-just-killed-prompt-engineering-with-8-words-and-i-cant-believe-it-worked-8349d6524d2b?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![The AI Bubble Is About To Burst, But The Next Bubble Is Already Growing](https://miro.medium.com/v2/resize:fit:679/format:webp/0*jQ7Z0Y2Rw8kblsEX)\n\n[![Will Lockett](https://miro.medium.com/v2/resize:fill:20:20/1*V0qWMQ8V5_NaF9yUoHAdyg.jpeg)](https://medium.com/@wlockett?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Will Lockett](https://medium.com/@wlockett?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**The AI Bubble Is About To Burst, But The Next Bubble Is Already Growing**\\\\\n\\\\\n**Techbros are preparing their latest bandwagon.**](https://medium.com/@wlockett/the-ai-bubble-is-about-to-burst-but-the-next-bubble-is-already-growing-383c0c0c7ede?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nSep 14, 2025\n\n[A clap icon21K\\\\\n\\\\\nA response icon918](https://medium.com/@wlockett/the-ai-bubble-is-about-to-burst-but-the-next-bubble-is-already-growing-383c0c0c7ede?source=post_page---read_next_recirc--044a2dfd9f62----0---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![Building the 7 Layers of a Production-Grade Agentic AI System](https://miro.medium.com/v2/resize:fit:679/format:webp/1*GB6tXauVBaHVGDE4L_FkYg.png)\n\n[![Level Up Coding](https://miro.medium.com/v2/resize:fill:20:20/1*5D9oYBd58pyjMkV_5-zXXQ.jpeg)](https://medium.com/gitconnected?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Level Up Coding](https://medium.com/gitconnected?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Fareed Khan](https://medium.com/@fareedkhandev?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**Building the 7 Layers of a Production-Grade Agentic AI System**\\\\\n\\\\\n**Service Layer, Middleware, Context Management and more**](https://medium.com/gitconnected/building-the-7-layers-of-a-production-grade-agentic-ai-system-37ee5d941f1c?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nDec 18, 2025\n\n[A clap icon1.5K\\\\\n\\\\\nA response icon21](https://medium.com/gitconnected/building-the-7-layers-of-a-production-grade-agentic-ai-system-37ee5d941f1c?source=post_page---read_next_recirc--044a2dfd9f62----1---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![What are Context Graphs: Building the AI that trulyUnderstands](https://miro.medium.com/v2/resize:fit:679/format:webp/0*KKPAVRJmt0f9PdOC)\n\n[![Neural Notions](https://miro.medium.com/v2/resize:fill:20:20/1*Xv_4SeKtNfCiqRNi_wDAoA.png)](https://medium.com/modelmind?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Neural Notions](https://medium.com/modelmind?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[Nikhil](https://medium.com/@nkwrites?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**What are Context Graphs: Building the AI that trulyUnderstands**\\\\\n\\\\\n**Imagine an AI system that not only processes information but truly understands it -> grasping the relationships between entities, the…**](https://medium.com/modelmind/what-are-context-graphs-building-the-ai-that-trulyunderstands-e7e5db39138d?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nDec 24, 2025\n\n[A clap icon33](https://medium.com/modelmind/what-are-context-graphs-building-the-ai-that-trulyunderstands-e7e5db39138d?source=post_page---read_next_recirc--044a2dfd9f62----2---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n![Git Confused Me for Years Until I Found This Simple Guide](https://miro.medium.com/v2/resize:fit:679/format:webp/1*YUALkK55VO_6mxjVqq_smQ.png)\n\n[![Let’s Code Future](https://miro.medium.com/v2/resize:fill:20:20/1*QXfeVFVbIzUGnlwXoOZvyQ.png)](https://medium.com/lets-code-future?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nIn\n\n[Let’s Code Future](https://medium.com/lets-code-future?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[The Unwritten Algorithm](https://medium.com/@the_unwritten_algorithm?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[**Git Confused Me for Years Until I Found This Simple Guide**\\\\\n\\\\\n**Most developers don’t really understand Git — here’s the simple truth.**](https://medium.com/lets-code-future/git-confused-me-for-years-until-i-found-this-simple-guide-a45223bebb40?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\nDec 19, 2025\n\n[A clap icon2.7K\\\\\n\\\\\nA response icon55](https://medium.com/lets-code-future/git-confused-me-for-years-until-i-found-this-simple-guide-a45223bebb40?source=post_page---read_next_recirc--044a2dfd9f62----3---------------------df603bb2_6a2f_4c4c_b7af_aa7ce3f42a0e--------------)\n\n[See more recommendations](https://medium.com/?source=post_page---read_next_recirc--044a2dfd9f62---------------------------------------)\n\n[Help](https://help.medium.com/hc/en-us?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Status](https://status.medium.com/?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[About](https://medium.com/about?autoplay=1&source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Press](mailto:pressinquiries@medium.com)\n\n[Blog](https://blog.medium.com/?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Rules](https://policy.medium.com/medium-rules-30e5502c4eb4?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----044a2dfd9f62---------------------------------------)\n\n[Text to speech](https://speechify.com/medium?source=post_page-----044a2dfd9f62---------------------------------------)\n\nreCAPTCHA\n\nRecaptcha requires verification.\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)\n\nprotected by **reCAPTCHA**\n\n[Privacy](https://www.google.com/intl/en/policies/privacy/) \\- [Terms](https://www.google.com/intl/en/policies/terms/)",
    "html": null,
    "raw_html": null,
    "json": null,
    "summary": null,
    "metadata": {
      "title": "Language Modelling on MPS using PyTorch | by Mridul Rao | Medium",
      "description": "Language Modelling on MPS using PyTorch Language modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human language accurately …",
      "url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
      "language": "en",
      "keywords": null,
      "robots": "index,noarchive,follow,max-image-preview:large",
      "og_title": "Language Modelling on MPS using PyTorch",
      "og_description": "Language modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human…",
      "og_url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
      "og_image": "https://miro.medium.com/v2/da:true/bc1f8416df0cad099e43cda2872716e5864f18a73bda2a7547ea082aca9b5632",
      "og_audio": null,
      "og_determiner": null,
      "og_locale": null,
      "og_locale_alternate": null,
      "og_site_name": "Medium",
      "og_video": null,
      "favicon": "https://miro.medium.com/v2/5d8de952517e8160e40ef9841c781cdc14a5db313057fa3c3de41c6f5b494b19",
      "dc_terms_created": null,
      "dc_date_created": null,
      "dc_date": null,
      "dc_terms_type": null,
      "dc_type": null,
      "dc_terms_audience": null,
      "dc_terms_subject": null,
      "dc_subject": null,
      "dc_description": null,
      "dc_terms_keywords": null,
      "modified_time": null,
      "published_time": "2024-03-23T18:13:00.346Z",
      "article_tag": null,
      "article_section": null,
      "source_url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
      "status_code": 200,
      "scrape_id": "019bfe68-6a67-72aa-9edf-85819f5be6d2",
      "num_pages": null,
      "content_type": "text/html; charset=utf-8",
      "proxy_used": "basic",
      "timezone": null,
      "cache_state": "hit",
      "cached_at": "2026-01-27T04:00:44.439Z",
      "credits_used": 1,
      "concurrency_limited": false,
      "concurrency_queue_duration_ms": null,
      "error": null,
      "og:image:alt": "Medium",
      "twitter:data1": "6 min read",
      "referrer": "unsafe-url",
      "al:ios:url": "medium://p/044a2dfd9f62",
      "al:web:url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
      "twitter:app:url:iphone": "medium://p/044a2dfd9f62",
      "al:android:package": "com.medium.reader",
      "twitter:card": "summary_large_image",
      "viewport": "width=device-width,minimum-scale=1,initial-scale=1,maximum-scale=1",
      "og:image": "https://miro.medium.com/v2/da:true/bc1f8416df0cad099e43cda2872716e5864f18a73bda2a7547ea082aca9b5632",
      "twitter:title": "Language Modelling on MPS using PyTorch",
      "al:ios:app_name": "Medium",
      "fb:app_id": "542599432471018",
      "og:type": "article",
      "author": "Mridul Rao",
      "al:android:app_name": "Medium",
      "article:published_time": "2024-03-23T18:13:00.346Z",
      "og:title": "Language Modelling on MPS using PyTorch",
      "twitter:site": "@Medium",
      "og:description": "Language modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human…",
      "al:ios:app_store_id": "828256236",
      "twitter:image:src": "https://miro.medium.com/v2/da:true/bc1f8416df0cad099e43cda2872716e5864f18a73bda2a7547ea082aca9b5632",
      "twitter:app:id:iphone": "828256236",
      "twitter:app:name:iphone": "Medium",
      "theme-color": "#000000",
      "al:android:url": "medium://p/044a2dfd9f62",
      "twitter:image:alt": "Medium",
      "article:author": "https://medium.com/@mridulrao674385",
      "og:url": "https://medium.com/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62",
      "og:site_name": "Medium",
      "apple-itunes-app": "app-id=828256236, app-argument=/@mridulrao674385/language-modelling-on-mps-using-pytorch-044a2dfd9f62, affiliate-data=pt=698524&ct=smart_app_banner&mt=8",
      "twitter:description": "Language modeling is a cornerstone of natural language processing (NLP), enabling machines to understand, generate, and interpret human…",
      "twitter:label1": "Reading time"
    },
    "links": null,
    "images": null,
    "screenshot": null,
    "actions": null,
    "warning": null,
    "change_tracking": null,
    "branding": null
  }
}